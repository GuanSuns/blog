<!DOCTYPE html>
<html>
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        RL Reading Note: Double DQN · Lin Guan&#39;s Personal Website
        
    </title>
    <link rel="icon" href= https://image-1252075188.cos.na-toronto.myqcloud.com/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/blog/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /blog/css/style.css?v=20180120 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/blog/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/blog/" >Lin Guan&#39;s Personal Website</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">RL Reading Note: Double DQN</a>
            </div>
    </div>
    
    <a class="home-link" href=/blog/>Lin Guan's Personal Website</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            RL Reading Note: Double DQN
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = Reinforcement Learning>Reinforcement Learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = Deep Learning>Deep Learning</a>
    
</div>
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = "/blog/static/api/js/share.js"];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2018/07/21</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="RL-Reading-Note-Dueling-Network-Architectures-for-Deep-Reinforcement-Learning"><a href="#RL-Reading-Note-Dueling-Network-Architectures-for-Deep-Reinforcement-Learning" class="headerlink" title="RL Reading Note: Dueling Network Architectures for Deep Reinforcement Learning"></a>RL Reading Note: Dueling Network Architectures for Deep Reinforcement Learning</h1><h2 id="1-Motivation"><a href="#1-Motivation" class="headerlink" title="1. Motivation"></a>1. Motivation</h2><p>For many states, it is unnecessary to estimate the value of each action choice, while for bootstrapping based algorithms, the estimation of state values is of great importance for every state. </p>
<h2 id="2-Traditional-neural-networks"><a href="#2-Traditional-neural-networks" class="headerlink" title="2. Traditional neural networks"></a>2. Traditional neural networks</h2><ul>
<li>Convolutional networks</li>
<li>MLPs</li>
<li>LSTMs</li>
<li>Auto-Encoders</li>
</ul>
<h2 id="3-Network-StructureL-Double-DQN-DDQN"><a href="#3-Network-StructureL-Double-DQN-DDQN" class="headerlink" title="3. Network StructureL Double DQN (DDQN)"></a>3. Network StructureL Double DQN (DDQN)</h2><p><img src="figure1.png" alt="
network-structure"></p>
<ul>
<li>Structure<ul>
<li>Two streams are combined via a special aggregating layer to produce an estimate fo the state-action value function Q: replace the traditional single-stream Q network in DQN with two streams</li>
<li>The lower layers stay the same as DQN while instead use two sequences of fully connected layers</li>
<li>Output: same as DQN, a set of Q values, one for each action</li>
</ul>
</li>
<li>Properties<ul>
<li>Produce separate estimates of state value function and advantage function</li>
<li>The dueling architecture can learn which states are (or are not) valuable , without having to learn the effect of each action for each state</li>
<li>Dueling architecture can more quickly identify the correct action during policy evaluation as redundant or similar actions are added to the learning problems</li>
<li>Use <strong>prioritized experience replay</strong> to increase the replay probabiliy of experience tuples that have a high expected learning progress (measured via TD-error)</li>
</ul>
</li>
</ul>
<h2 id="4-Update-Rules"><a href="#4-Update-Rules" class="headerlink" title="4. Update Rules"></a>4. Update Rules</h2><ul>
<li><strong>Advantage function</strong>: <script type="math/tex">A^{\pi}(s, a) = Q^{\pi} (s, a) - V^{\pi} (s)</script> <script type="math/tex">V^{\pi} (s) = E_{a \sim \pi(s)} [Q^{\pi} (s, a)]</script> <script type="math/tex">E_{a \sim \pi(s)} [A^{\pi} (s, a)] = 0</script></li>
<li>Intuitively, the ad vantage function subtracts the value of the state from the Q function to obtain a relative measure of the importance of each action.</li>
<li><strong>Update DDQN</strong>: <script type="math/tex">L_i(\theta _i) = E_{s,a,r,s \prime}[(y_i ^{DDQN} - Q(s,a; \theta  _i)^2]</script> with <script type="math/tex">y_i ^{DDQN} = r + \gamma Q(s \prime , arg max_{a \prime} Q (s \prime, a \prime; \theta _i); \theta ^{-})</script></li>
<li>Combine Advantage function and alue funtion as well as address the <strong>Issue of identifiability</strong> (force the advantage function estimator to have zero advantage at chosen action):<script type="math/tex; mode=display">Q(s, a; \theta, \alpha, \beta) = V(s; \theta, \beta) + (A(s, a; \theta, \alpha) - \frac{1}{|A|} \sum _{a \prime \in |A|} A(s, a \prime ; \theta, \alpha)</script></li>
</ul>
<h2 id="5-Understand-dueling-network-architecture"><a href="#5-Understand-dueling-network-architecture" class="headerlink" title="5. Understand dueling network architecture"></a>5. Understand dueling network architecture</h2><p><img src="figure2.png" alt="figure-2
"></p>
<ul>
<li>Dueling architecture is particularly useful in states where its actions do not affect the environment in any (or much) relevant way : figure 2 (top) shows that the value network strea pays attention to the road and the score while the advantage stream does not pay much attention to the visual input because its action choice is practically irrelevant when there are no cars in front. </li>
<li>Use average operator instead of max in formula (9) stabilize the optimization, even though it becomes off-target by a constant, but the advantages now only need to change as fast as the mean instead of having to compensate any change to the optimal action in (8).</li>
<li>The advantage of the dueling architecture lies partly in its ability to learn the state-value function efficiently. With every update of the Q-values, the value stream V is updated (while in DQN, only a specific Q value is updated everytime).</li>
</ul>
<h2 id="6-Comparison-to-DQN"><a href="#6-Comparison-to-DQN" class="headerlink" title="6. Comparison to DQN"></a>6. Comparison to DQN</h2><ul>
<li>DQN uses single-stream Q network</li>
<li>DQN takes the state representation as a single input, and generates the weights of different action as output.</li>
<li>In Q-learning and DQN, the ma operator uses the same values to both select and evaluate an action, which can lead to <strong>overoptimistic value estimates</strong></li>
<li>DDQN has more frequent updates on state values, thus allows for better approximation of state values.</li>
<li>In DQN, in some states, the average action is roughly 0.4, whereas the average state value across those states is about 15. This <strong>difference in scales</strong> can lead to small amounts of noise in the updates can lead to great reorderings of the actions. DDQN is more robust in this case.</li>
</ul>
<h2 id="7-Inplementations-and-Experiments"><a href="#7-Inplementations-and-Experiments" class="headerlink" title="7. Inplementations and Experiments"></a>7. Inplementations and Experiments</h2><ul>
<li><strong>Corridor Environment</strong><ul>
<li>Behavior policy: $\epsilon$ - greedy with $\epsilon$ = 0.001</li>
<li>Three layers: first hidden layer of 50 units, two layer MLP with 25 hidden units</li>
<li>When we increase the number of actions, DDQN works better</li>
<li>The stream $V(s; \theta, \beta)$ learns a general value that is shared across many similar actions at s, hence leading to <strong>faster convergence</strong>.</li>
</ul>
</li>
<li><strong>Atari Game</strong><ul>
<li>Same low-level convolutional structure as DQN: 3 convolutional layers followed by 2 fully-connected layer.</li>
<li>Three convolutional layers: first convolutional layer: 32 $8 \times 8$ filters with stride 4; second: 64 $4 \times 4$ filters with stride 2; third: 64 $3 \times 3$ filters with stride 1.</li>
<li>Value and advantage streams both have a fully connected layer with 512 units.</li>
<li>Combine value and advantage streams using equation (9).</li>
<li><strong>Slightly lower the learning rate</strong> since it can deteriorate the performance.</li>
<li><strong>Rescale the combined gradient</strong> entering the last convolutional layer by $1 / \sqrt 2$ , which <strong>increases stability</strong>.</li>
<li><strong>Clip the gradients</strong> to have their norm less than or equal to 10.</li>
<li><strong>Single Clip</strong> as comparison (isolate the contributions of the dueling architecture.</li>
<li>Random starting position.</li>
<li><strong>Evaluation</strong>: improvement in percentage in score over the better of human and baseline agent scores</li>
<li><strong>Human Starts Evaluation</strong>: prevent the agent from simply remebering a sequence of actions</li>
<li>To avoid <strong>adverse interactions</strong> , use $6.25 * 10^{-5}$ for the learning rate and 10 for the gradient clipping norm.</li>
</ul>
</li>
</ul>
<h2 id="8-Algorithm"><a href="#8-Algorithm" class="headerlink" title="8. Algorithm"></a>8. Algorithm</h2><p><img src="figure3.png" alt="figure-3
"></p>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/blog/22/Deep-RL-from-Human-Preferences/" title= RL Reading Note: Deep RL from Human Preferences >
                    <span>Next Post</span>
                    <span>RL Reading Note: Deep RL from Human Preferences</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/blog/11/RL-Reading-Note-Deep-COACH/" title= RL Reading Note: Deep COACH >
                    <span>Previous Post</span>
                    <span>RL Reading Note: Deep COACH</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:guanlin@utexas.edu" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/GuanSuns" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#RL-Reading-Note-Dueling-Network-Architectures-for-Deep-Reinforcement-Learning"><span class="toc-number">1.</span> <span class="toc-text">RL Reading Note: Dueling Network Architectures for Deep Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Motivation"><span class="toc-number">1.1.</span> <span class="toc-text">1. Motivation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Traditional-neural-networks"><span class="toc-number">1.2.</span> <span class="toc-text">2. Traditional neural networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Network-StructureL-Double-DQN-DDQN"><span class="toc-number">1.3.</span> <span class="toc-text">3. Network StructureL Double DQN (DDQN)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Update-Rules"><span class="toc-number">1.4.</span> <span class="toc-text">4. Update Rules</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Understand-dueling-network-architecture"><span class="toc-number">1.5.</span> <span class="toc-text">5. Understand dueling network architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Comparison-to-DQN"><span class="toc-number">1.6.</span> <span class="toc-text">6. Comparison to DQN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-Inplementations-and-Experiments"><span class="toc-number">1.7.</span> <span class="toc-text">7. Inplementations and Experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-Algorithm"><span class="toc-number">1.8.</span> <span class="toc-text">8. Algorithm</span></a></li></ol></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 26 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span><a class="archive-post-title" href= "/blog/13/RL-Reading-Note/" >RL Reading Note</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Transformers-from-Scratch/" >Transformers from Scratch</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span><a class="archive-post-title" href= "/blog/24/Leveraging-Human-Guidance-for-Deep-Reinforcement-Learning-Tasks/" >Leveraging Human Guidance for Deep Reinforcement Learning Tasks</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Papers-RL-with-Human-Feedback/" >Papers: RL with Human Feedback</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/18</span><a class="archive-post-title" href= "/blog/18/RL-Reading-Note-Relevant-Papers/" >Relevant Papers: RL with Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/blog/23/Relevant-Papers/" >Relevant Papers</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/22</span><a class="archive-post-title" href= "/blog/22/Deep-RL-from-Human-Preferences/" >RL Reading Note: Deep RL from Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/21</span><a class="archive-post-title" href= "/blog/21/RL-Reading-Note-Double-DQN/" >RL Reading Note: Double DQN</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Reading-Note-Deep-COACH/" >RL Reading Note: Deep COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Meeting-Note-TAMER-versus-COACH/" >RL Meeting Note: TAMER versus COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/10</span><a class="archive-post-title" href= "/blog/10/RL-Reading-Note-Covergent-Actor-Critic-by-Humans-COACH/" >RL Reading Note: Covergent Actor-Critic by Humans (COACH)</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/08</span><a class="archive-post-title" href= "/blog/08/RL-Note-Chapter-12-Eligibility-Traces/" >RL Reading Note: Chapter 12 Eligibility Traces</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/09</span><a class="archive-post-title" href= "/blog/09/RL-Note-Chapter-4-DP/" >RL Reading Note: Chapter 4 DP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/blog/06/Final-Entry/" >Final Entry</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span><a class="archive-post-title" href= "/blog/15/CS373-Week-12/" >CS373 Week 12</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span><a class="archive-post-title" href= "/blog/08/CS373-Week-11/" >CS373 Week 11</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/01</span><a class="archive-post-title" href= "/blog/01/CS373-Week-10/" >CS373 Week 10</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-9/" >CS373 Week 9</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-8/" >CS373 Week 8</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-7/" >CS373 Week 7</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-6/" >CS373 Week 6</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/18</span><a class="archive-post-title" href= "/blog/18/CS373-Week-5/" >CS373 Week 5</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-4/" >CS373 Week 4</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-3/" >CS373 Week 3</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/28</span><a class="archive-post-title" href= "/blog/28/CS373-Week-2/" >CS373 Week 2</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span><a class="archive-post-title" href= "/blog/21/CS373-Week-1/" >CS373 Week 1</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">CS373</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human-in-the-Loop</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Reinforcement Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">IJCAI Survey 2019</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Deep Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Preferences Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human Feedback</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">COACH</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/blog/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


