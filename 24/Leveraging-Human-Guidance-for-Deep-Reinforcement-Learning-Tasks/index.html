<!DOCTYPE html>
<html>
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        Leveraging Human Guidance for Deep Reinforcement Learning Tasks · Lin Guan&#39;s Personal Website
        
    </title>
    <link rel="icon" href= https://image-1252075188.cos.na-toronto.myqcloud.com/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/blog/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /blog/css/style.css?v=20180120 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/blog/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/blog/" >Lin Guan&#39;s Personal Website</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Leveraging Human Guidance for Deep Reinforcement Learning Tasks</a>
            </div>
    </div>
    
    <a class="home-link" href=/blog/>Lin Guan's Personal Website</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Leveraging Human Guidance for Deep Reinforcement Learning Tasks
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = Human-in-the-Loop>Human-in-the-Loop</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = Reinforcement Learning>Reinforcement Learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = IJCAI Survey 2019>IJCAI Survey 2019</a>
    
</div>
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = "/blog/static/api/js/share.js"];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2019/06/24</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="Interactive-Teaching-Strategies-for-Agent-Training"><a href="#Interactive-Teaching-Strategies-for-Agent-Training" class="headerlink" title="Interactive Teaching Strategies for Agent Training"></a>Interactive Teaching Strategies for Agent Training</h1><ul>
<li><strong>Goal</strong>: They show that using a joint decision-making approach, the amont of attention required from the teacher can be reduced compared to teacher-based approaches. </li>
<li><strong>Approaches</strong><ul>
<li>Joint-initiated teaching strategies: the student decides whether to ask for the teacher’s attention based on one of the student-initiated approaches for asking for advice, then the teacher decides whether to provide advice based on one of the teacher-initiated advising approaches.d</li>
<li>Advise Important requires little attention but wastes advice budget; Correct Important requires more attention but saves advice budget for important states (better performance)</li>
</ul>
</li>
<li><strong>Experimental environment</strong>: Pac-Man game<ul>
<li>Making mistakes in Pac-Man game is particularly harmful $\rightarrow$ similar to semi-autonomous driving.</li>
</ul>
</li>
<li><strong>Future directions</strong>:<ul>
<li>Importance heuristic will likely not generalize well to domains where unrecoverable mistakes are rare</li>
<li>methods for generalizing the teacher’s advice beyond a specific state</li>
</ul>
</li>
</ul>
<h1 id="Agent-Agnostic-Human-in-the-Loop-Reinforcement-Learning"><a href="#Agent-Agnostic-Human-in-the-Loop-Reinforcement-Learning" class="headerlink" title="Agent-Agnostic Human-in-the-Loop Reinforcement Learning"></a>Agent-Agnostic Human-in-the-Loop Reinforcement Learning</h1><h1 id="Policy-Shaping-with-Human-Teacher"><a href="#Policy-Shaping-with-Human-Teacher" class="headerlink" title="Policy Shaping with Human Teacher"></a>Policy Shaping with Human Teacher</h1><ul>
<li><strong>Goal</strong><ul>
<li>Find out what happens when data is generated by human teachers.</li>
<li>Understand the consequences of interpreting a teacher’s silence as an action choice evaluation.</li>
</ul>
</li>
<li><strong>Interpretation of humam feedback: evaluating action choices</strong><ul>
<li>In direct constrast to the reward shaping algorithms where human feedback is interpreted as an estimation of the sum of future discounted reward.</li>
</ul>
</li>
<li><strong>Concepts mentioned in Related Work</strong><ul>
<li>Evaluation Channel</li>
<li>Human guidance</li>
<li>Policy shaping</li>
</ul>
</li>
<li><strong>Experiment Result</strong><ul>
<li>Human teachers outperformed the simulated teacher because they were able to recognize many different ways of winning.</li>
<li>When to give positive, negative, or no feedback have a significant impact on data quality.</li>
<li>Different iterpretations of silence can increase or decrease performance (e.g. silence might be inpterpreted as ok action).</li>
<li>It’s interesting to find that there is good performance when teachers are biased towards silence meaning bad, but the learner assumes silence to mean good, despite bias and assumptions being miss matched.</li>
</ul>
</li>
</ul>
<h1 id="Reinforcement-Learning-from-Human-Preferences"><a href="#Reinforcement-Learning-from-Human-Preferences" class="headerlink" title="Reinforcement Learning from Human Preferences"></a>Reinforcement Learning from Human Preferences</h1><ul>
<li><strong>Motivation</strong><ul>
<li>Sometimes no reward function is available</li>
<li>Need to decrease the number of feedback: using human feedback directly as a reward function is prohibitively expensive for RL</li>
<li>It’s easier for humans to provide consistent comparisons than consistent absolute scores</li>
<li>Maybe: scale human feedback up to deep RL and to learn much more complex behavior</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>Two ways to evaluate the behavior</strong></p>
<ul>
<li><p><strong>Quantitative</strong>: preferences $\succ$ can be generated by reward function r, then the agent will seek to maximize the total rewards.  <script type="math/tex">((o_0^1, a_0^1), ..., (o_{k-1}^1, a_{k-1}^1)) \succ ((o_0^2, a_0^2), ..., (o_{k-1}^2, a_{k-1}^2))</script> whenever <script type="math/tex">r(o_0^1, a_0^1) +  ... +  r(o_{k-1}^1, a_{k-1}^1)\succ r(o_0^2, a_0^2) + ... + r(o_{k-1}^2, a_{k-1}^2)</script></p>
</li>
<li><p><strong>Qualitative</strong>: sometimes we don’t have reward function to quantitatively evaluate the behaviors. In this case, all people can do is qualitatively evaluate how well the agent satisfies the human’s preferences.</p>
</li>
</ul>
</li>
<li><p><strong>Assumptions</strong></p>
<ul>
<li>There is a human overseer who can express preferences between trajectory segments.</li>
<li>it’s not assumed that people can reset the system to an arbitrary state (no random start)</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Algorithm (asynchronous)</strong><ul>
<li><strong>Optimize the policy</strong><ul>
<li>Use the learned reward function $\hat{r}(o_t, a_t)$ to optimize the policy. </li>
<li>Can use any policy gradient methods like A2C or TRPO.</li>
<li>Normalize the rewards produced by $\hat{r}(o_t, a_t)$ to have zero mean and constant std.</li>
</ul>
</li>
<li><strong>Preference Elicitation</strong><ul>
<li>Record human judgment $(\sigma^{1}, \sigma^{2}, \mu)$</li>
</ul>
</li>
<li><strong>Fitting the Reward Function</strong><ul>
<li>Loss function: </li>
<li>Fit an ensemble of predictors: independently normalize each of these predictors and then average the results</li>
<li>Prevent overfitting: use $l^2$ regularization (keep validation loss between 1.1 and 1.5 times the training loss) and apply dropout.</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Implementations and Experiments</strong><ul>
<li>Parameters:<ul>
<li>Human feedbak: 1-2 sentence description</li>
<li>Length of each trajectory segment: 1 to 2 seconds</li>
</ul>
</li>
<li>Result:<ul>
<li>Perform poorly on QBert but good on Enduro</li>
</ul>
</li>
<li>Understand the result:<ul>
<li>Training with learned reward functions tends to be <strong>less stable and higher variance</strong>, while having a <strong>comparable mean performance</strong>.</li>
<li>By 1400 labels, the algorithm performs slightly better than if it had simply been give the true reward, perhaps because the <strong>learned reward function is slightly better shaped</strong>: the reward learning procedure assigns <strong>positive rewards</strong> to all behaviors that typically followed by high <strong>true reward</strong></li>
<li>Human feedback works better when it serves as <strong>bonus reward</strong></li>
<li><strong>Issue</strong>: <strong>Short clips in Qbert can be confusing and hard to evaluate</strong></li>
<li>For continuous control tasks, predicting comparisons work much better than predicting scores</li>
<li>Ask humans to compare longer clips is significantly more helpful per clip and significantly less helpful per frame: it is easier to compare <strong>longer clips</strong> because the provide <strong>more context</strong> than single frames.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="A-Social-Reinforcement-Learning-Agent"><a href="#A-Social-Reinforcement-Learning-Agent" class="headerlink" title="A Social Reinforcement Learning Agent"></a>A Social Reinforcement Learning Agent</h1><ul>
<li><strong>Robot in online chatting rooms LambdaMOO</strong>: Cobot</li>
<li><strong>Key Ideas</strong><ul>
<li>Reward Shaping: Having separate reward functions for each user is a way of asserting the importance of user identity to the learning process.</li>
<li>We can maintain separate staet spaces as well, in the hopes of simplifying states and thus speeding learning.</li>
<li>Linear Function Approximator: from RL perspective, they maintain a vector of real-valued weights indexed by the possible actions. A positive weight for some action means that the feature increases the probability of taking that action, while a negative weight decreases the probability .</li>
</ul>
</li>
<li><strong>Experiment Results and Findings</strong><ul>
<li>Learning must be quick and significant: user fickleness (preferences are not stationary) and data sparisity (inadepuate feedback).</li>
<li>Reward is not an adepuate metric for the performance: the average cumulative reward received by Cobot actually goes down, since people tend to give less feedback as time goes by.</li>
<li>A small group of users have highly-uniform policy, which means their preferences are independent of states (e.g. User M seemed to have a strong preference for an action called Roll-Call)</li>
</ul>
</li>
</ul>
<h1 id="Hierarchical-Imitation-and-Reinforcement-Learning"><a href="#Hierarchical-Imitation-and-Reinforcement-Learning" class="headerlink" title="Hierarchical Imitation and Reinforcement Learning"></a>Hierarchical Imitation and Reinforcement Learning</h1><h1 id="Reinforcement-Learning-from-Simultaneous-Human-and-MDP-Reward"><a href="#Reinforcement-Learning-from-Simultaneous-Human-and-MDP-Reward" class="headerlink" title="Reinforcement Learning from Simultaneous Human and MDP Reward"></a>Reinforcement Learning from Simultaneous Human and MDP Reward</h1><ul>
<li><strong>Combination Techniques</strong>:<ul>
<li><strong>Reward Shaping</strong>: R’(s,a)=R(s,a)+beta*H(s,a)</li>
<li><strong>Q augmentation</strong>: Q’(s,a) = Q(s,a) + beta*H(s,a)</li>
<li><strong>Action Biasing</strong>: Q’(s,a) = Q(s,a) + beta*H(s,a) only during action selection</li>
<li><strong>Control Sharing</strong>: $P(a=argmax_a[H(s,a)] ) = min(\beta , 1)$, otherwise use base RL agent’s action selection mechanism</li>
<li>Note that $\beta$ is a predefined combination parameter and is annealed by a predefined factor after each episode .</li>
</ul>
</li>
<li><p><strong>Experiment</strong></p>
<ul>
<li>Expect the greatest gains to come from training near the beginning of learning.</li>
<li>Desirable characteristics: steady behavior, responsiveness to the trainer, trainer can give feedback to the MDP-only policy, traner’s influence is applied appropriately (<strong>Eligibility Module</strong>: larger influence in omre recently trained space and less in less recently)<ul>
<li>H’s influence should increase in areas of the state-action space with recent human training but not in areas that have not been targeted with feedback and decrease in the absence of traning, leaving the set of optimal policies unchanged in the limit.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Conclusion</strong></p>
<ul>
<li>Sequential TAMER+RL: The more a technique directly affects action selection, the better it does. Action biasing &gt; Control sharing &gt; Q aug &gt; Sarsa &gt; Reward shaping</li>
<li>Simultaneous TAMER+RL: human influence is more effective at the begining (compared to RL); the later training (human join traning after n episodes) group is better (using the same amount of human feedback).</li>
</ul>
</li>
</ul>
<h1 id="Policy-Shaping-Integrating-Human-Feedback-with-Reinforcement-Learning"><a href="#Policy-Shaping-Integrating-Human-Feedback-with-Reinforcement-Learning" class="headerlink" title="Policy Shaping: Integrating Human Feedback with Reinforcement Learning"></a>Policy Shaping: Integrating Human Feedback with Reinforcement Learning</h1><ul>
<li><strong>Interpretation</strong><ul>
<li>Define human feedback as ADVICE, the probility (s,a) is optimal is:<script type="math/tex; mode=display">\frac{C^{\Delta _{s,a}}}{C^{\Delta _{s,a}} + (1-C) ^ {\Delta _{s,a}}}</script>where $\Delta$ is the difference between the number of “right” and “wrong” labels, C is the probility that a human might make mistake.</li>
<li>THe probability of performing s,a according to the feedback policy $\pi _ E$ is <script type="math/tex; mode=display">C^{\Delta _{s,a}} + (1-C) ^ {\sum_{j \neq a} \Delta _{s,j}}</script></li>
</ul>
</li>
<li><strong>How to use ADVICE</strong><ul>
<li>BQL (a kind of Q value) + ADVICE</li>
</ul>
</li>
<li><strong>Experiment</strong><ul>
<li>ADVICE is <strong>more robust to noisy signal</strong> from human (inconsistent feedback).</li>
<li>Underestimation of C: reduce effectiveness.</li>
<li>Overestimation of C: might lead to a suboptimal policy.</li>
<li>ADVICE is better in many cases because their use of human influence parameters is not disconnected from the amount of information in the accumulated feedback.</li>
</ul>
</li>
</ul>
<h1 id="Interactive-Learning-from-Policy-Dependent-Human-Feedback"><a href="#Interactive-Learning-from-Policy-Dependent-Human-Feedback" class="headerlink" title="Interactive Learning from Policy-Dependent Human Feedback"></a>Interactive Learning from Policy-Dependent Human Feedback</h1>
    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/blog/02/Transformers-from-Scratch/" title= Transformers from Scratch >
                    <span>Next Post</span>
                    <span>Transformers from Scratch</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/blog/02/Papers-RL-with-Human-Feedback/" title= Papers: RL with Human Feedback >
                    <span>Previous Post</span>
                    <span>Papers: RL with Human Feedback</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:guanlin@utexas.edu" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/GuanSuns" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Interactive-Teaching-Strategies-for-Agent-Training"><span class="toc-number">1.</span> <span class="toc-text">Interactive Teaching Strategies for Agent Training</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Agent-Agnostic-Human-in-the-Loop-Reinforcement-Learning"><span class="toc-number">2.</span> <span class="toc-text">Agent-Agnostic Human-in-the-Loop Reinforcement Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Policy-Shaping-with-Human-Teacher"><span class="toc-number">3.</span> <span class="toc-text">Policy Shaping with Human Teacher</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reinforcement-Learning-from-Human-Preferences"><span class="toc-number">4.</span> <span class="toc-text">Reinforcement Learning from Human Preferences</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#A-Social-Reinforcement-Learning-Agent"><span class="toc-number">5.</span> <span class="toc-text">A Social Reinforcement Learning Agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hierarchical-Imitation-and-Reinforcement-Learning"><span class="toc-number">6.</span> <span class="toc-text">Hierarchical Imitation and Reinforcement Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Reinforcement-Learning-from-Simultaneous-Human-and-MDP-Reward"><span class="toc-number">7.</span> <span class="toc-text">Reinforcement Learning from Simultaneous Human and MDP Reward</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Policy-Shaping-Integrating-Human-Feedback-with-Reinforcement-Learning"><span class="toc-number">8.</span> <span class="toc-text">Policy Shaping: Integrating Human Feedback with Reinforcement Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Interactive-Learning-from-Policy-Dependent-Human-Feedback"><span class="toc-number">9.</span> <span class="toc-text">Interactive Learning from Policy-Dependent Human Feedback</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 27 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/blog/23/Reading-Notes-Explainable-RL/" >Reading Notes: Explainable RL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span><a class="archive-post-title" href= "/blog/13/RL-Reading-Note/" >RL Reading Note</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Transformers-from-Scratch/" >Transformers from Scratch</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span><a class="archive-post-title" href= "/blog/24/Leveraging-Human-Guidance-for-Deep-Reinforcement-Learning-Tasks/" >Leveraging Human Guidance for Deep Reinforcement Learning Tasks</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Papers-RL-with-Human-Feedback/" >Papers: RL with Human Feedback</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/18</span><a class="archive-post-title" href= "/blog/18/RL-Reading-Note-Relevant-Papers/" >Relevant Papers: RL with Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/blog/23/Relevant-Papers-Preference-Based-RL/" >Relevant Papers: Preference-based RL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/22</span><a class="archive-post-title" href= "/blog/22/Deep-RL-from-Human-Preferences/" >RL Reading Note: Deep RL from Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/21</span><a class="archive-post-title" href= "/blog/21/RL-Reading-Note-Double-DQN/" >RL Reading Note: Double DQN</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Reading-Note-Deep-COACH/" >RL Reading Note: Deep COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Meeting-Note-TAMER-versus-COACH/" >RL Meeting Note: TAMER versus COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/10</span><a class="archive-post-title" href= "/blog/10/RL-Reading-Note-Covergent-Actor-Critic-by-Humans-COACH/" >RL Reading Note: Covergent Actor-Critic by Humans (COACH)</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/08</span><a class="archive-post-title" href= "/blog/08/RL-Note-Chapter-12-Eligibility-Traces/" >RL Reading Note: Chapter 12 Eligibility Traces</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/09</span><a class="archive-post-title" href= "/blog/09/RL-Note-Chapter-4-DP/" >RL Reading Note: Chapter 4 DP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/blog/06/Final-Entry/" >Final Entry</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span><a class="archive-post-title" href= "/blog/15/CS373-Week-12/" >CS373 Week 12</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span><a class="archive-post-title" href= "/blog/08/CS373-Week-11/" >CS373 Week 11</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/01</span><a class="archive-post-title" href= "/blog/01/CS373-Week-10/" >CS373 Week 10</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-9/" >CS373 Week 9</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-8/" >CS373 Week 8</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-7/" >CS373 Week 7</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-6/" >CS373 Week 6</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/18</span><a class="archive-post-title" href= "/blog/18/CS373-Week-5/" >CS373 Week 5</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-4/" >CS373 Week 4</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-3/" >CS373 Week 3</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/28</span><a class="archive-post-title" href= "/blog/28/CS373-Week-2/" >CS373 Week 2</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span><a class="archive-post-title" href= "/blog/21/CS373-Week-1/" >CS373 Week 1</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">CS373</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human-in-the-Loop</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Reinforcement Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">IJCAI Survey 2019</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human Feedback</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Deep Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Preferences Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">COACH</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Explainable RL</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">XAI</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/blog/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


