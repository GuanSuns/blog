<!DOCTYPE html>
<html>
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        Relevant Papers: Preference-based RL · Lin Guan&#39;s Personal Website
        
    </title>
    <link rel="icon" href= https://image-1252075188.cos.na-toronto.myqcloud.com/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/blog/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /blog/css/style.css?v=20180120 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/blog/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/blog/" >Lin Guan&#39;s Personal Website</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Relevant Papers: Preference-based RL</a>
            </div>
    </div>
    
    <a class="home-link" href=/blog/>Lin Guan's Personal Website</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Relevant Papers: Preference-based RL
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = Reinforcement Learning>Reinforcement Learning</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = Preferences Learning>Preferences Learning</a>
    
</div>
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = "/blog/static/api/js/share.js"];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2018/07/23</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h1 id="Relevant-Papers"><a href="#Relevant-Papers" class="headerlink" title="Relevant Papers"></a>Relevant Papers</h1><h2 id="1-Preference-Based-RL"><a href="#1-Preference-Based-RL" class="headerlink" title="1. Preference-Based RL"></a>1. Preference-Based RL</h2><h3 id="1-1-Motivation"><a href="#1-1-Motivation" class="headerlink" title="1.1. Motivation"></a>1.1. Motivation</h3><p>we can directly learn from an expert’s preferences instead of a hand-designed numeric reward (agent’s performance can be very sensitive to the used numeric values)</p>
<h3 id="1-2-Problems"><a href="#1-2-Problems" class="headerlink" title="1.2. Problems:"></a>1.2. Problems:</h3><ul>
<li>Do not explicit state the assumption</li>
<li>Lack coherent framework</li>
<li>How to express pairwise comparisons</li>
<li>Partial order: incomparable pairs (or difference can be infinitesimal)</li>
</ul>
<h3 id="1-3-Main-approaches"><a href="#1-3-Main-approaches" class="headerlink" title="1.3. Main approaches"></a>1.3. Main approaches</h3><h4 id="1-3-1-Types-of-preferences"><a href="#1-3-1-Types-of-preferences" class="headerlink" title="1.3.1. Types of preferences"></a>1.3.1. Types of preferences</h4><ul>
<li>state</li>
<li>action</li>
<li>trajectory</li>
</ul>
<h4 id="1-3-2-Approaches"><a href="#1-3-2-Approaches" class="headerlink" title="1.3.2. Approaches"></a>1.3.2. Approaches</h4><ul>
<li><strong>Learn a utility functions</strong><ul>
<li>surrogate function is not directly comparable to an approximated reward or return function because it may be subject to concept drift if the estimate of the expert’s optimality criterion can change over time. </li>
</ul>
</li>
<li><strong>Learn a policy</strong>: <ul>
<li>induce a distribution over a parametric policy space</li>
<li>compare and rank policies</li>
</ul>
</li>
<li><strong>Learn a preference model</strong></li>
</ul>
<h3 id="1-4-Action-Preferences"><a href="#1-4-Action-Preferences" class="headerlink" title="1.4. Action Preferences"></a>1.4. Action Preferences</h3><h4 id="1-4-1-Notes"><a href="#1-4-1-Notes" class="headerlink" title="1.4.1. Notes"></a>1.4.1. Notes</h4><ul>
<li>feedback concerning the long-term return should be preferred: but this requires expert to be really familiar with the expected long-term outcome</li>
</ul>
<h4 id="1-4-2-Papers"><a href="#1-4-2-Papers" class="headerlink" title="1.4.2. Papers"></a>1.4.2. Papers</h4><ul>
<li>J. Fürnkranz, E. Hüllermeier, W. Cheng, and S.-H. Park. <strong>Preference-based reinforcement learning: A formal framework and a policy iteration algorithm</strong>. Machine Learning, 89 (1-2):123–156, 2012. Special Issue of Selected Papers from ECML/PKDD-11<ul>
<li><a href="https://link.springer.com/content/pdf/10.1007%2Fs10994-012-5313-8.pdf" target="_blank" rel="noopener">https://link.springer.com/content/pdf/10.1007%2Fs10994-012-5313-8.pdf</a></li>
<li><strong>Learn a preference model</strong></li>
</ul>
</li>
<li>W. B. Knox and P. Stone. <strong>Reinforcement learning from simultaneous human and MDP reward</strong>. In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS-12), pages 475–482. 2012.<ul>
<li><a href="https://pdfs.semanticscholar.org/4503/09925f91da287e7227c2034be1a119997e41.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/4503/09925f91da287e7227c2034be1a119997e41.pdf</a></li>
</ul>
</li>
<li>E. Hüllermeier, J. Fürnkranz, W. Cheng, and K. Brinker. <strong>Label ranking by learning pairwise preferences</strong>. Artificial Intelligence, 172(16–17):1897–1916, 2008.<ul>
<li><a href="http://weiweicheng.com/research/papers/cheng-ai08.pdf" target="_blank" rel="noopener">http://weiweicheng.com/research/papers/cheng-ai08.pdf</a></li>
<li>Combined classifiers via voting or weighted voting</li>
<li><strong>Learn a preference model</strong></li>
</ul>
</li>
</ul>
<h3 id="1-5-State-Preferences"><a href="#1-5-State-Preferences" class="headerlink" title="1.5. State Preferences"></a>1.5. State Preferences</h3><h4 id="1-5-1-Notes"><a href="#1-5-1-Notes" class="headerlink" title="1.5.1. Notes"></a>1.5.1. Notes</h4><ul>
<li>more informative than action preferences because they define relations between parts of the global state space</li>
<li>but also suffer from the <strong>long-term/short-term optimality problem</strong></li>
<li>State preferences are <strong>slightly less demanding for the expert</strong> as it is not required to compare actions for a given state. However, the expert still needs to estimate the future outcome of the policy for a given state.</li>
<li><strong>Short-term optimality</strong>: Short-term state preferences do not define a trade-off, because it is unclear whether visiting an undominated state once should be preferred over visiting a rarely dominated state multiple times.</li>
</ul>
<h4 id="1-5-2-Papers"><a href="#1-5-2-Papers" class="headerlink" title="1.5.2. Papers"></a>1.5.2. Papers</h4><ul>
<li><strong>Short-term</strong>: (Optional, Robot Locomotion)M. Zucker, J. A. Bagnell, C. Atkeson, and J. Kuffner, Jr. <strong>An optimization approach to rough terrain locomotion</strong>. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA-10), pages 3589–3595. 2010.<ul>
<li><a href="http://www.cs.cmu.edu/~cga/papers/zucker-icra10.pdf" target="_blank" rel="noopener">http://www.cs.cmu.edu/~cga/papers/zucker-icra10.pdf</a></li>
</ul>
</li>
<li>C. Wirth and J. Fürnkranz. <strong>First steps towards learning from game annotations</strong>. In Proceedings of the ECAI Workshop on Preference Learning: Problems and Applications in AI, pages 53–58, 2012.<ul>
<li><a href="http://www.ke.tu-darmstadt.de/events/PL-12/papers/11-wirth.pdf" target="_blank" rel="noopener">http://www.ke.tu-darmstadt.de/events/PL-12/papers/11-wirth.pdf</a></li>
<li>Use human annotations</li>
<li><strong>Learn value-based utility: state-preference</strong></li>
</ul>
</li>
</ul>
<h3 id="1-6-Trajectory-Preferences"><a href="#1-6-Trajectory-Preferences" class="headerlink" title="1.6. Trajectory Preferences"></a>1.6. Trajectory Preferences</h3><h4 id="1-6-1-Notes"><a href="#1-6-1-Notes" class="headerlink" title="1.6.1. Notes"></a>1.6.1. Notes</h4><ul>
<li>The <strong>least demanding</strong> preferences type for the expert as she can directly evaluate the outcomes of full trajectories</li>
<li><strong>Temporal credit assignment problem</strong>: need to determine which states or actions are responsible for the encountered </li>
<li>In practice, almost no algorithm known to the authors (PbRL Survey) is capable of dealing with preferences between trajectories with different initial states. (<strong>Deep RL from Human Preferences</strong> uses trajectory segments with different initial states)</li>
</ul>
<h4 id="1-6-2-Papers"><a href="#1-6-2-Papers" class="headerlink" title="1.6.2. Papers"></a>1.6.2. Papers</h4><ul>
<li><strong>A Bayesian Approach for Policy Learning from Trajectory Preference Queries</strong>, NIPS 2012<ul>
<li><a href="http://papers.nips.cc/paper/4805-a-bayesian-approach-for-policy-learning-from-trajectory-preference-queries.pdf" target="_blank" rel="noopener">http://papers.nips.cc/paper/4805-a-bayesian-approach-for-policy-learning-from-trajectory-preference-queries.pdf</a></li>
<li><strong>Induce a distribution over a parametric policy space</strong></li>
<li>Generate trajectories pair using ROLLOUT</li>
</ul>
</li>
<li>R. Busa-Fekete, B. Szörényi, P. Weng, W. Cheng, and E. Hüllermeier. <strong>Preference-based evolutionary direct policy search</strong>. In Proceedings of the ICRA Workshop on Autonomous Learning, 2013.<ul>
<li><a href="http://www.ecmlpkdd2013.org/wp-content/uploads/2013/09/PBRL_08-BusaFekete.pdf" target="_blank" rel="noopener">http://www.ecmlpkdd2013.org/wp-content/uploads/2013/09/PBRL_08-BusaFekete.pdf</a></li>
<li><strong>Compare and rank policies</strong></li>
<li>Generate trajectories pair using ROLLOUT</li>
<li>optimization can be performed with algorithms similar to <em>evolutionary direct policy search</em> (EDPS; Heidrich Meisner and Igel 2009)</li>
</ul>
</li>
<li>C. Wirth, J. Fürnkranz, and G. Neumann. <strong>Model-Free Preference-based Reinforcement Learning</strong> . In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16), pages 2222–2228, 2016.<ul>
<li><a href="https://ewrl.files.wordpress.com/2015/02/ewrl12_2015_submission_10.pdf" target="_blank" rel="noopener">https://ewrl.files.wordpress.com/2015/02/ewrl12_2015_submission_10.pdf</a></li>
<li><strong>Learn non-linear utility function</strong></li>
</ul>
</li>
<li><strong>Deep Reinforcement Learning from Human Preferences</strong>, Paul F. Christiano, NIPS 2017<ul>
<li><a href="http://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences" target="_blank" rel="noopener">http://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences</a></li>
<li><strong>approximate a non-linear utility function</strong></li>
<li><strong>Learn Reward-Based utility</strong></li>
</ul>
</li>
<li>Kupcsik, D. Hsu, and W. S. Lee. <strong>Learning dynamic robot-to-human object handover from human feedback</strong>. In Proceedings of the 17th International Symposium on Robotics Research (ISRR-15), 2015.<ul>
<li><a href="https://arxiv.org/pdf/1603.06390.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06390.pdf</a></li>
<li><strong>Learn return-based utility</strong></li>
</ul>
</li>
<li>C. Wirth and J. Fürnkranz. <strong>A policy iteration algorithm for learning from preference-based feedback</strong>. In Advances in Intelligent Data Analysis XII: 12th International Symposium (IDA-13), volume 8207 of LNCS, pages 427–437. 2013a.<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-3-642-41398-8_37" target="_blank" rel="noopener">https://link.springer.com/chapter/10.1007/978-3-642-41398-8_37</a></li>
<li>Derive additional action preferences for intermediate states in trajectory preferences, defining an approximate solution to the temporal credit assignment problem.</li>
</ul>
</li>
</ul>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
            <li class="next">
                <a href= "/blog/18/RL-Reading-Note-Relevant-Papers/" title= Relevant Papers: RL with Human Preferences >
                    <span>Next Post</span>
                    <span>Relevant Papers: RL with Human Preferences</span>
                </a>
            </li>
        
        
            <li class="previous">
                <a href= "/blog/22/Deep-RL-from-Human-Preferences/" title= RL Reading Note: Deep RL from Human Preferences >
                    <span>Previous Post</span>
                    <span>RL Reading Note: Deep RL from Human Preferences</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:guanlin@utexas.edu" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/GuanSuns" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Relevant-Papers"><span class="toc-number">1.</span> <span class="toc-text">Relevant Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Preference-Based-RL"><span class="toc-number">1.1.</span> <span class="toc-text">1. Preference-Based RL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Motivation"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.1. Motivation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Problems"><span class="toc-number">1.1.2.</span> <span class="toc-text">1.2. Problems:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Main-approaches"><span class="toc-number">1.1.3.</span> <span class="toc-text">1.3. Main approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-Types-of-preferences"><span class="toc-number">1.1.3.1.</span> <span class="toc-text">1.3.1. Types of preferences</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-Approaches"><span class="toc-number">1.1.3.2.</span> <span class="toc-text">1.3.2. Approaches</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-Action-Preferences"><span class="toc-number">1.1.4.</span> <span class="toc-text">1.4. Action Preferences</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-1-Notes"><span class="toc-number">1.1.4.1.</span> <span class="toc-text">1.4.1. Notes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-4-2-Papers"><span class="toc-number">1.1.4.2.</span> <span class="toc-text">1.4.2. Papers</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-State-Preferences"><span class="toc-number">1.1.5.</span> <span class="toc-text">1.5. State Preferences</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-1-Notes"><span class="toc-number">1.1.5.1.</span> <span class="toc-text">1.5.1. Notes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-5-2-Papers"><span class="toc-number">1.1.5.2.</span> <span class="toc-text">1.5.2. Papers</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-Trajectory-Preferences"><span class="toc-number">1.1.6.</span> <span class="toc-text">1.6. Trajectory Preferences</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-1-Notes"><span class="toc-number">1.1.6.1.</span> <span class="toc-text">1.6.1. Notes</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-6-2-Papers"><span class="toc-number">1.1.6.2.</span> <span class="toc-text">1.6.2. Papers</span></a></li></ol></li></ol></li></ol></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 27 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/blog/23/Reading-Notes-Explainable-RL/" >Reading Notes: Explainable RL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span><a class="archive-post-title" href= "/blog/13/RL-Reading-Note/" >RL Reading Note</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Transformers-from-Scratch/" >Transformers from Scratch</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span><a class="archive-post-title" href= "/blog/24/Leveraging-Human-Guidance-for-Deep-Reinforcement-Learning-Tasks/" >Leveraging Human Guidance for Deep Reinforcement Learning Tasks</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Papers-RL-with-Human-Feedback/" >Papers: RL with Human Feedback</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/18</span><a class="archive-post-title" href= "/blog/18/RL-Reading-Note-Relevant-Papers/" >Relevant Papers: RL with Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/blog/23/Relevant-Papers-Preference-Based-RL/" >Relevant Papers: Preference-based RL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/22</span><a class="archive-post-title" href= "/blog/22/Deep-RL-from-Human-Preferences/" >RL Reading Note: Deep RL from Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/21</span><a class="archive-post-title" href= "/blog/21/RL-Reading-Note-Dueling-DQN/" >RL Reading Note: Dueling DQN</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Reading-Note-Deep-COACH/" >RL Reading Note: Deep COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Meeting-Note-TAMER-versus-COACH/" >RL Meeting Note: TAMER versus COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/10</span><a class="archive-post-title" href= "/blog/10/RL-Reading-Note-Covergent-Actor-Critic-by-Humans-COACH/" >RL Reading Note: Covergent Actor-Critic by Humans (COACH)</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/08</span><a class="archive-post-title" href= "/blog/08/RL-Note-Chapter-12-Eligibility-Traces/" >RL Reading Note: Chapter 12 Eligibility Traces</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/09</span><a class="archive-post-title" href= "/blog/09/RL-Note-Chapter-4-DP/" >RL Reading Note: Chapter 4 DP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/blog/06/Final-Entry/" >Final Entry</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span><a class="archive-post-title" href= "/blog/15/CS373-Week-12/" >CS373 Week 12</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span><a class="archive-post-title" href= "/blog/08/CS373-Week-11/" >CS373 Week 11</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/01</span><a class="archive-post-title" href= "/blog/01/CS373-Week-10/" >CS373 Week 10</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-9/" >CS373 Week 9</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-8/" >CS373 Week 8</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-7/" >CS373 Week 7</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-6/" >CS373 Week 6</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/18</span><a class="archive-post-title" href= "/blog/18/CS373-Week-5/" >CS373 Week 5</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-4/" >CS373 Week 4</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-3/" >CS373 Week 3</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/28</span><a class="archive-post-title" href= "/blog/28/CS373-Week-2/" >CS373 Week 2</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span><a class="archive-post-title" href= "/blog/21/CS373-Week-1/" >CS373 Week 1</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">CS373</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Deep Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Reinforcement Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Preferences Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human-in-the-Loop</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">IJCAI Survey 2019</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human Feedback</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">COACH</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Explainable RL</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">XAI</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/blog/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


