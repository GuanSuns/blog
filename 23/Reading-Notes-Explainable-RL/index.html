<!DOCTYPE html>
<html>
    <head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" >
    <title>
        
        Reading Notes: Explainable RL · Lin Guan&#39;s Personal Website
        
    </title>
    <link rel="icon" href= https://image-1252075188.cos.na-toronto.myqcloud.com/favicon.ico>
    <!-- TODO: 在font-face加载完毕后改变字体  -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/webfont/1.6.28/webfontloader.js"></script>
    <!-- 提前加载place holder  -->
    <style type="text/css">
        @font-face {
            font-family: 'Oswald-Regular';
            src: url(/blog/font/Oswald-Regular.ttf);
        }
    </style>
    <style type="text/css">
        .site-intro {
            position: relative;
            width: 100%;
            height: 50vh;
            overflow: hidden;
            box-shadow: -0.1rem 0 0.5rem 0 rgba(0, 0, 0, 0.5);
        }
        .site-intro-placeholder {
            position: absolute;
            z-index: -2;
            top: 0;
            left: 0px;
            width: calc(100% + 300px);
            height: 100%;
            background: repeating-linear-gradient(-45deg, #444 0, #444 80px, #333 80px, #333 160px);
            background-position: center center;
            transform: translate3d(-226px, 0, 0);
            animation: gradient-move 2.5s ease-out 0s 1;
        }
        @keyframes gradient-move {
        0% {
            transform: translate3d(-226px, 0, 0);
        }
        100% {
            transform: translate3d(0, 0, 0);
        }
    }
</style>
    <link rel="stylesheet" href = /blog/css/style.css?v=20180120 />
    <script src="//cdn.staticfile.org/jquery/3.2.1/jquery.min.js" defer></script>
    
    <script src="/blog/scripts/main.js" defer></script>
    <!-- 百度统计  -->
    
    <!-- 谷歌统计  --><!-- hexo-inject:begin --><!-- hexo-inject:end -->
    
</head>

    
        <body class="post-body">
    
    
<!-- hexo-inject:begin --><!-- hexo-inject:end --><header class="header">

    <div class="read-progress"></div>
    <div class="header-sidebar-menu">&#xe775;</div>
    <!-- post页的toggle banner  -->
    
    <div class="banner">
            <div class="blog-title">
                <a href="/blog/" >Lin Guan&#39;s Personal Website</a>
            </div>
            <div class="post-title">
                <a href="#" class="post-name">Reading Notes: Explainable RL</a>
            </div>
    </div>
    
    <a class="home-link" href=/blog/>Lin Guan's Personal Website</a>
</header>
    <div class="wrapper">
        <div class="site-intro">
    
    <!-- 主页  -->
    
    
    <!-- 404页  -->
            
    <div class="site-intro-img" style="background-image: url(/blog/intro/post-bg.jpg)"></div>
    <div class="site-intro-placeholder"></div>
    <div class="site-intro-meta">
        <!-- 标题  -->
        <h1 class="intro-title">
            <!-- 主页  -->
            
            Reading Notes: Explainable RL
            <!-- 404 -->
            
        </h1>
        <!-- 副标题 -->
        <p class="intro-subtitle">
            <!-- 主页副标题  -->
            
            
            <!-- 404 -->
            
        </p>
        <!-- 文章页meta -->
        
            <!-- 文章页标签  -->
            
                <div class= post-intro-tags >
    
        <a class="post-tag" href="javascript:void(0);" data-href = Explainable RL>Explainable RL</a>
    
        <a class="post-tag" href="javascript:void(0);" data-href = XAI>XAI</a>
    
</div>
            
            <script>window._bd_share_config = { "common": { "bdSnsKey": {}, "bdText": "", "bdMini": "2", "bdMiniList": false, "bdPic": "", "bdStyle": "1", "bdSize": "16" }, "share": {} }; with (document) 0[(getElementsByTagName('head')[0] || body).appendChild(createElement('script')).src = "/blog/static/api/js/share.js"];</script>
            <div class="post-intro-meta">
                <span class="post-intro-calander iconfont-archer">&#xe676;</span>
                <span class="post-intro-time">2019/09/23</span>
                <span id="busuanzi_container_page_pv" class="busuanzi-pv">
                    <span class="iconfont-archer">&#xe604;</span>
                    <span id="busuanzi_value_page_pv"></span>
                </span>
                <span class="shareWrapper">
                    <span class="iconfont-archer shareIcon">
                        &#xe601;
                    </span>
                    <span class="bdsharebuttonbox">
                        <a href="#" class="bds_more shareText" data-cmd="more">Share</a>
                    </span>
                </span>
            </div>
        
    </div>
</div>
        <img class="loading" src="/blog/assets/loading.svg" style="display: block; margin: 6rem auto 0 auto; width: 6rem; height: 6rem;" />
        <script>
            var browser = {
                    versions: function () {
                        var u = window.navigator.userAgent;
                        return {
                            userAgent: u,
                            trident: u.indexOf('Trident') > -1, //IE内核
                            presto: u.indexOf('Presto') > -1, //opera内核
                            webKit: u.indexOf('AppleWebKit') > -1, //苹果、谷歌内核
                            gecko: u.indexOf('Gecko') > -1 && u.indexOf('KHTML') == -1, //火狐内核
                            mobile: !!u.match(/AppleWebKit.*Mobile.*/), //是否为移动终端
                            ios: !!u.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/), //ios终端
                            android: u.indexOf('Android') > -1 || u.indexOf('Linux') > -1, //android终端或者uc浏览器
                            iPhone: u.indexOf('iPhone') > -1 || u.indexOf('Mac') > -1, //是否为iPhone或者安卓QQ浏览器
                            iPad: u.indexOf('iPad') > -1, //是否为iPad
                            webApp: u.indexOf('Safari') == -1, //是否为web应用程序，没有头部与底部
                            weixin: u.indexOf('MicroMessenger') == -1, //是否为微信浏览器
                            uc: u.indexOf('UCBrowser') > -1 //是否为android下的UC浏览器
                        };
                    }()
                }

            function fontLoaded(){
                console.log('font loaded');
                if (document.getElementsByClassName('site-intro-meta')) {
                    document.getElementsByClassName('intro-title')[0].classList.add('intro-fade-in');
                    document.getElementsByClassName('intro-subtitle')[0].classList.add('intro-fade-in');
                    var postIntroTags = document.getElementsByClassName('post-intro-tags')[0],
                        postIntroMeat = document.getElementsByClassName('post-intro-meta')[0];
                        if (postIntroTags) {
                            postIntroTags.classList.add('post-fade-in');
                        }
                        if (postIntroMeat) {
                            postIntroMeat.classList.add('post-fade-in');
                        }
                    }
                }
                
            console.log("userAgent:" + browser.versions.userAgent);
            // UC不支持跨域，所以直接显示
            if (browser.versions.uc) {
                console.log("UCBrowser");
                fontLoaded();
            } else {
                WebFont.load({
                    custom: {
                        families: ['Oswald-Regular']
                    },
                    loading: function () {  //所有字体开始加载
                        // console.log('loading');
                    },
                    active: function () {  //所有字体已渲染
                        fontLoaded();
                    },
                    inactive: function () { //字体预加载失败，无效字体或浏览器不支持加载
                        console.log('inactive: timeout');
                        fontLoaded();
                    },
                    timeout: 7000 // Set the timeout to two seconds
                });
            }
        </script>
        <div class="container container-unloaded">
            <main class="main post-page">
    <article class="article-entry">
        <h2 id="Generation-of-Policy-Level-Explanations-for-Reinforcement-Learning"><a href="#Generation-of-Policy-Level-Explanations-for-Reinforcement-Learning" class="headerlink" title="Generation of Policy-Level Explanations for Reinforcement Learning"></a>Generation of Policy-Level Explanations for Reinforcement Learning</h2><ul>
<li><strong>Assumptions</strong><ul>
<li>all states consist of an assignment to features</li>
<li>Only consider the deterministic case</li>
<li>Agent behaves similarly in a state from the set (abstract states) in a similar fashion</li>
</ul>
</li>
<li><strong>Methods</strong><ul>
<li>Use the Feature Importance Ranking Measure (FIRM) to group states from an original MDP into abstract states (<strong>Interchangeability</strong>):  $I_f(c)$  represents the importance of feature f; If $f$ takes on the same value for all states s in c or its value does not influence the system’s output, then f is not important.</li>
<li>Each abstract state represents a set of grounded states from the original MDP.</li>
<li>Explaination: create a local explanation consisting of the set of features which are important in that state</li>
</ul>
</li>
<li><strong>Generated Explaination</strong><ul>
<li><img src="https://lh3.googleusercontent.com/KD5DZFBAUTLTgftPwqaLJ-ISIUjLOND1xJucGkcQTah72R8pislvPWpDY4ORvnfr8edxMVi1n9BI" alt="Abstract Policy Graph"></li>
</ul>
</li>
<li><strong>Experiment Result</strong><ul>
<li>Even with only 10% of the states, the prediction is correct over 93% of the time. When given 80% of the states, predictions are correct 98.7% of the time for both the stochastic and deterministic environment, which suggests that the model is able to identify genuine patterns in the policy.</li>
<li>Errors the system makes are probably caused by the splitting order induced by APG Gen’s greedy splitting strategy.</li>
</ul>
</li>
<li><strong>Some Notes</strong><ul>
<li>No user study</li>
<li><strong>This algorithm provides an explaination incorporating expected future actions</strong></li>
<li>This algorithem is compatible with most existing RL systems that learn Q-function (<strong>Probably not compatible with RL systems use deep network</strong>)</li>
<li>Only presents a MDP of human-designed binary features to the user (not a clear explaination)</li>
<li>It’s not actually an explaination, especially in terms of explaining the intention or motivation of an action </li>
<li>It only considers <strong>binary boolean features</strong>.</li>
<li>When generating the abstrat state, they use binary features to split the set of states into two parts repeatly until all features have low importance. However, when making a decision, the humans probably won’t just take one factor (feature) into consideration (In their experiment, one action only changes one feature/factor).</li>
</ul>
</li>
</ul>
<h2 id="Graying-the-black-box-Understanding-DQNs"><a href="#Graying-the-black-box-Understanding-DQNs" class="headerlink" title="Graying the black box: Understanding DQNs"></a>Graying the black box: Understanding DQNs</h2><ul>
<li><p><strong>Methods</strong></p>
<ul>
<li><strong>Feature extraction</strong>: statistics from the game such as Q values estimates, generation time, the termination signal, reward and played action; also use hand-crafted features (useful)</li>
<li><strong>t-SNE</strong>: Apply the t-SNE algorithm directly on the collected neural activations.</li>
<li><strong>Saliency maps</strong>: display the corresponding Saliency maps</li>
<li><strong>Analyze the policy manually</strong>: Embed states into a space where states within certain regions of this space behave similarly</li>
</ul>
</li>
<li><p><strong>Semi Aggregated MDP (SAMDP)</strong></p>
<ul>
<li>Feature selection</li>
<li>Aggregation via Spatio-temporal clustering: The goal of Aggregation is to find a mapping(clustering) from the MDP feature space to the <strong>AMDP</strong> state space C. Use an extension of the K-means algorithm to enforces temporal coherency along trajectories.</li>
<li>Skill identification: at time t the agent enters an AMDP state $c_i$ at an MDP state, it chooses a skill according to its SAMDP policy.</li>
<li>Inference: given the SAMDP states and skills, we infer the skill length, the SAMDP reward and the SAMDP probability transition matrix from observations.</li>
<li>Model selection: there are different hyper parameter to<br>tune, such as the number of SAMDP states (K) and the window size (w) for the clustering algorithm; follow the Occams Razor principle, and aim to find the simplest model which best explains the data; use Value Mean Square Error(VMSE), measures the consistency of the model with the observations.</li>
</ul>
</li>
<li><strong>Experiment Result</strong><ul>
<li>The SAMDP algorithm managed to identify most of the policy hierarchy automatically and the clusters are consistent with the manual clusters.</li>
</ul>
</li>
<li><strong>Some Notes</strong>:<ul>
<li><strong>Spatial abstraction</strong> aims to group states that share common attributes such as similar policy or value function; <strong>temporal abstarction</strong> aims to group states in which the agent to plan with skills (also known as options, macro-actions and temporally-extended actions)</li>
<li>It uses hand-crafted features but lack of explanation of the reasoning behind the decision made by the DQN.</li>
<li>It does something like summarizaion (off-line) after the training process.</li>
<li>The t-SNE provides some high-level explaination of the policy. However, it still needs human to interpret the relation between different cluster (like figuring out which feature matters. The SAMDP does help to explain the policy but still need furture research on finding how the DQN agent balance between different factors (features).</li>
</ul>
</li>
</ul>
<h2 id="Distilling-Deep-Reinforcement-Learning-Policies-in-Soft-Decision-Trees"><a href="#Distilling-Deep-Reinforcement-Learning-Policies-in-Soft-Decision-Trees" class="headerlink" title="Distilling Deep Reinforcement Learning Policies in Soft Decision Trees"></a>Distilling Deep Reinforcement Learning Policies in Soft Decision Trees</h2><ul>
<li><strong>Methods</strong><ul>
<li>Soft Decision Trees (SDTs), which proposed by Frosst and Hinton in 2017, are a hybrid classification model of binary decision trees with a predetermined depth and neural networks.</li>
<li>Distill the policy learned by a deep reinforcement learning into soft decision trees.</li>
<li>By examining the learned filters along the traversed path from the root to leaf, one can thus under- stand which features the soft decision tree considers to assign a particular action distribution to a particular state.</li>
</ul>
</li>
<li><strong>Generated Explainations</strong><ul>
<li><img src="https://lh3.googleusercontent.com/7HfIW3dewA8HsNdU7--H_J7_8JGBZjrVEvYrkHW7NFZWPj2Psu44wC2H27xwCfLkcQaERGPfIS78" alt="Generated Explaination" title="Generated Explaination"></li>
</ul>
</li>
<li><strong>Some Notes</strong><ul>
<li>The essence of soft decision trees is that this kind of model does not rely on hierarchical features to classify data, but <strong>instead relies on hierarchical decisions</strong>.</li>
<li>Understanding why a particular state observation leads to a certain action distribution can be realized by examining how the input state itself is filtered along the path between the root and the chosen leaf node in the soft decision tree model</li>
<li>Problem: there is a descrepancy between Q-values and the tree (which means the SDT fails to perfectly represent the learned policy)</li>
<li>still require the prior-knowledge of human to understand the association amaong selected features. Still need human tofigure out the consequence/intent/motivation of an action given the saliency map (This can be a problem when the domain is complex)</li>
<li>Problem: the depth of tree is a parameter that need to be tuned.</li>
</ul>
</li>
</ul>
<h2 id="Particle-Swarm-Optimization-for-Generating-Interpretable-Fuzzy-Reinforcement-Learning"><a href="#Particle-Swarm-Optimization-for-Generating-Interpretable-Fuzzy-Reinforcement-Learning" class="headerlink" title="Particle Swarm Optimization for Generating Interpretable Fuzzy Reinforcement Learning"></a>Particle Swarm Optimization for Generating Interpretable Fuzzy Reinforcement Learning</h2><ul>
<li><strong>Goals</strong><ul>
<li>Solve prblems in domains that online learning are prohibited</li>
</ul>
</li>
<li><strong>Methods</strong><ul>
<li>Learn a world dynamic model g with dataset D that contains the state transitions</li>
<li>During optimization, each particle’s position x in the PSO represents a parameterization fo the fuzzy policy $\pi [x]$</li>
<li>Search for optimal parameters for (one half of) the fuzzy policy rule</li>
<li>Use PSO because the population-based optimizer does not require any gradient information </li>
</ul>
</li>
<li><strong>Generated Explainations</strong><ul>
<li>Fuzzy rules: one example can be IF cart=right slope AND speed= high right THEN accelerate=positive</li>
</ul>
</li>
<li><strong>Some Notes</strong>:<ul>
<li>Need to model the world dynamic</li>
<li>Need to tune the number of rules (not self-adjustable)</li>
<li>It only learns a rule-representation of the policy but can’t apply to any RL method based on Q-functions. Besides, it does not show the potential consequence of an action (it does not reflect an intelligent agent’s intent, performance, future plans, and reasoning process)</li>
</ul>
</li>
</ul>
<h2 id="Transparency-and-Explanation-in-Deep-Reinforcement-Learning-Neural-Networks"><a href="#Transparency-and-Explanation-in-Deep-Reinforcement-Learning-Neural-Networks" class="headerlink" title="Transparency and Explanation in Deep Reinforcement Learning Neural Networks"></a>Transparency and Explanation in Deep Reinforcement Learning Neural Networks</h2><ul>
<li><strong>Assumptions</strong><ul>
<li>Q(s, a) can be approximated with a linear function $Q(s,a) = w^{T}s + b$ where w is the derivation of Q(s,a) with respect to the state image.</li>
</ul>
</li>
<li><strong>Methods</strong><ul>
<li>corporate object features and object valence, i.e. positive or negative influence in an agent’s decisions, into DRLN architectures</li>
<li>Use Template Matching to recognize objects in the game image</li>
<li>Use object channels to incorporate features of objects </li>
<li>To calculate the derivation w of Q(s,a) with respect to the state image: $w = Q(s, a)-Q(s_o, a)$, $s_o$ is the state in which we mask an object with background color</li>
</ul>
</li>
<li><strong>Generated Explainations</strong><ul>
<li><img src="https://lh3.googleusercontent.com/-BY00PkI5jIaiM8NE5PKX3s1tSgEvZviyq93zESRCyJoLxfO_Lg3BPeuQV7A7erHj0FB_Mwt7Sya" alt="Object Saliency Map"></li>
</ul>
</li>
<li><strong>Some Notes</strong>:<ul>
<li><strong>Agent transparency</strong> is the quality of an interface (e.g. visual, linguistic) pertaining to its abilities to afford an operators comprehension about an intelligent agent’s intent, performance, future plans, and reasoning process</li>
<li>Ease of transparency seems to be inversely proportional to agent sophistication</li>
<li>“(Iyer et al. 2018) leverage these pixel-level explanations and use an object detector to produce <strong>object saliency maps</strong>, which are explanations for the behavior of deep RL agents in terms of objects. Unlike our method, their method explains a single decision without the context of potential future decisions.”</li>
</ul>
</li>
</ul>
<h2 id="Minimal-sufficient-explanations-for-factored-markov-decision-processes"><a href="#Minimal-sufficient-explanations-for-factored-markov-decision-processes" class="headerlink" title="Minimal sufficient explanations for factored markov decision processes"></a>Minimal sufficient explanations for factored markov decision processes</h2><ul>
<li><strong>Assumptions</strong><ul>
<li>Might need a set of explainations to justify the optimal action rather than just 1</li>
</ul>
</li>
<li><strong>Methods</strong><ul>
<li>Introduced the concept of a minimal sufficient explanation through which an action can be explained using the fewest possible terms</li>
<li>$V^{\pi^{<em>}} \ge V_{MSE} \ge Q^{\pi^{</em>}(s_0, a)}$<br>  for any action a not equals $\pi^{*}(s_0)$.</li>
</ul>
</li>
<li><strong>Some Notes</strong><ul>
<li>Produce contrastive explanations which compare the agent’s action to a proposed alternative, but require a known factored MDP (Learning model of the environment may be more complicated than the learned policy)</li>
<li>Problem: it uses a fixed set (3) of templates to generate explaination</li>
<li>Need the MDP designer to provide reward functions and transition model, which will be used to genreate the optimal policy using value-iteration. (Injecting domain-specific information requires additional efforts)</li>
<li>The format of explaination is interesting, since it indicates the potential outcome of an action (with “probability” comparing to other actions). One example is Action TakeCS343&amp;CS448 is the best action because it is likely to take you to CoursesCompleted = 6, TermNumber = Final about 0.86 times, which is as high as any other action.</li>
</ul>
</li>
</ul>
<h2 id="Toward-Interpretable-Deep-Reinforcement-Learning-with-Linear-Model-U-Trees"><a href="#Toward-Interpretable-Deep-Reinforcement-Learning-with-Linear-Model-U-Trees" class="headerlink" title="Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees"></a>Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees</h2><ul>
<li><strong>Methods</strong><ul>
<li>The first mimic learning framework for Q functions in DRL: trains an interpretable (simpler) mimic model to match the predictions of a highly accurate model</li>
<li>U-tree is a classic online reinforcement learning method which represents a Q function using a tree structure. LMUT is a U-Tree with a linear model to each leaf node</li>
<li>Use feature influence to split the tree: evaluate the influence of a splitting feature by the total variance reduction of the Q values</li>
</ul>
</li>
<li><strong>Generated Explainations</strong><ul>
<li>Example in the mountain car: when velocity in [-0.003, -0.001] and position in [-0.42, 0.13], then Q = &lt;-31.2, -29.6, -29.4&gt;</li>
</ul>
</li>
<li><strong>Some Notes</strong><ul>
<li>It learns a U-Tree representing the Q-function and the MDP model</li>
<li>Very Interesting finding: <strong>The attention of DNN would change with time, so a single saliency map is not enough to explain the entire reasoning process</strong>. Example here, the first image is often used to locate the pipes (obstacles) and the bird, while the remaining three images provide further information about the bird’s location and velocity. (<strong>As the same, explaination generated by a single state/image is not enought to represent the entire reasoning process. The first images at each row carry most information</strong>)<br><img src="https://lh3.googleusercontent.com/UvJ63KL1IV-ewbUedVKhQxonHdtzkgznG58yeznIgHmqX3RhdsfBWcktLVmn0IvmVgn6ibzUcmPr" alt="The attention of DNN would change with time, so a single saliency map is not enough to explain the entire reasoning process"></li>
</ul>
</li>
</ul>

    </article>
    <!-- 前后页  -->
    <ul class="post-pager">
        
        
            <li class="previous">
                <a href= "/blog/13/RL-Reading-Note/" title= RL Reading Note >
                    <span>Previous Post</span>
                    <span>RL Reading Note</span>
                </a>
            </li>
        
    </ul>
    <!-- 评论插件 -->
    <!-- 来必力City版安装代码 -->

<!-- City版安装代码已完成 -->
    
    
    <!--PC版-->

    <!--PC版-->


    
    <!-- 评论 -->
</main>
            <!-- profile -->
            
        </div>
        <footer class="footer footer-unloaded">
    <!-- social  -->
    
    <div class="social">
        
    
        
            
                <a href="mailto:guanlin@utexas.edu" class="iconfont-archer email" title=email ></a>
            
        
    
        
            
                <a href="//github.com/GuanSuns" class="iconfont-archer github" target="_blank" title="github"></a>
            
        
    

    </div>
    
    <!-- powered by Hexo  -->
    <div class="copyright">
        <span id="hexo-power">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a></span><span class="iconfont-archer power">&#xe635;</span><span id="theme-info">Theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank">archer</a></span>
    </div>
    <!-- 不蒜子  -->
    
</footer>
    </div>
    <!-- toc -->
    
    <div class="toc-wrapper">
        <div class="toc-catalog">
            <span class="iconfont-archer catalog-icon">&#xe613;</span><span>CATALOG</span>
        </div>
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Generation-of-Policy-Level-Explanations-for-Reinforcement-Learning"><span class="toc-number">1.</span> <span class="toc-text">Generation of Policy-Level Explanations for Reinforcement Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Graying-the-black-box-Understanding-DQNs"><span class="toc-number">2.</span> <span class="toc-text">Graying the black box: Understanding DQNs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Distilling-Deep-Reinforcement-Learning-Policies-in-Soft-Decision-Trees"><span class="toc-number">3.</span> <span class="toc-text">Distilling Deep Reinforcement Learning Policies in Soft Decision Trees</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Particle-Swarm-Optimization-for-Generating-Interpretable-Fuzzy-Reinforcement-Learning"><span class="toc-number">4.</span> <span class="toc-text">Particle Swarm Optimization for Generating Interpretable Fuzzy Reinforcement Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transparency-and-Explanation-in-Deep-Reinforcement-Learning-Neural-Networks"><span class="toc-number">5.</span> <span class="toc-text">Transparency and Explanation in Deep Reinforcement Learning Neural Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Minimal-sufficient-explanations-for-factored-markov-decision-processes"><span class="toc-number">6.</span> <span class="toc-text">Minimal sufficient explanations for factored markov decision processes</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Toward-Interpretable-Deep-Reinforcement-Learning-with-Linear-Model-U-Trees"><span class="toc-number">7.</span> <span class="toc-text">Toward Interpretable Deep Reinforcement Learning with Linear Model U-Trees</span></a></li></ol>
    </div>
    
    <div class="back-top">&#xe639;</div>
    <div class="sidebar">
    <div class="sidebar-header sidebar-header-show-archive">
        <div class="sidebar-category">
            <span class="sidebar-archive-link"><span class="iconfont-archer">&#xe67d;</span>Archive</span>
            <span class="sidebar-tags-link"><span class="iconfont-archer">&#xe610;</span>Tag</span>
        </div>
    </div>
    <div class="sidebar-content sidebar-content-show-archive">
          <div class="sidebar-archive">
    <!-- 在ejs中将archive按照时间排序 -->
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    <div class="total-archive"> Total : 27 </div>
    
    <div class="post-archive">
    
    
    
    
    <div class="archive-year"> 2019 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/23</span><a class="archive-post-title" href= "/blog/23/Reading-Notes-Explainable-RL/" >Reading Notes: Explainable RL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/13</span><a class="archive-post-title" href= "/blog/13/RL-Reading-Note/" >RL Reading Note</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Transformers-from-Scratch/" >Transformers from Scratch</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/24</span><a class="archive-post-title" href= "/blog/24/Leveraging-Human-Guidance-for-Deep-Reinforcement-Learning-Tasks/" >Leveraging Human Guidance for Deep Reinforcement Learning Tasks</a>
        </li>
    
    
    
    
    
        </ul>
    
    <div class="archive-year"> 2018 </div>
    <ul class="year-list">
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">09/02</span><a class="archive-post-title" href= "/blog/02/Papers-RL-with-Human-Feedback/" >Papers: RL with Human Feedback</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">08/18</span><a class="archive-post-title" href= "/blog/18/RL-Reading-Note-Relevant-Papers/" >Relevant Papers: RL with Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/23</span><a class="archive-post-title" href= "/blog/23/Relevant-Papers-Preference-Based-RL/" >Relevant Papers: Preference-based RL</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/22</span><a class="archive-post-title" href= "/blog/22/Deep-RL-from-Human-Preferences/" >RL Reading Note: Deep RL from Human Preferences</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/21</span><a class="archive-post-title" href= "/blog/21/RL-Reading-Note-Double-DQN/" >RL Reading Note: Double DQN</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Reading-Note-Deep-COACH/" >RL Reading Note: Deep COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/11</span><a class="archive-post-title" href= "/blog/11/RL-Meeting-Note-TAMER-versus-COACH/" >RL Meeting Note: TAMER versus COACH</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/10</span><a class="archive-post-title" href= "/blog/10/RL-Reading-Note-Covergent-Actor-Critic-by-Humans-COACH/" >RL Reading Note: Covergent Actor-Critic by Humans (COACH)</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">07/08</span><a class="archive-post-title" href= "/blog/08/RL-Note-Chapter-12-Eligibility-Traces/" >RL Reading Note: Chapter 12 Eligibility Traces</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">06/09</span><a class="archive-post-title" href= "/blog/09/RL-Note-Chapter-4-DP/" >RL Reading Note: Chapter 4 DP</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">05/06</span><a class="archive-post-title" href= "/blog/06/Final-Entry/" >Final Entry</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/15</span><a class="archive-post-title" href= "/blog/15/CS373-Week-12/" >CS373 Week 12</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/08</span><a class="archive-post-title" href= "/blog/08/CS373-Week-11/" >CS373 Week 11</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">04/01</span><a class="archive-post-title" href= "/blog/01/CS373-Week-10/" >CS373 Week 10</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-9/" >CS373 Week 9</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-8/" >CS373 Week 8</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">03/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-7/" >CS373 Week 7</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/25</span><a class="archive-post-title" href= "/blog/25/CS373-Week-6/" >CS373 Week 6</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/18</span><a class="archive-post-title" href= "/blog/18/CS373-Week-5/" >CS373 Week 5</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/11</span><a class="archive-post-title" href= "/blog/11/CS373-Week-4/" >CS373 Week 4</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">02/04</span><a class="archive-post-title" href= "/blog/04/CS373-Week-3/" >CS373 Week 3</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/28</span><a class="archive-post-title" href= "/blog/28/CS373-Week-2/" >CS373 Week 2</a>
        </li>
    
    
        <li class="archive-post-item">
            <span class="archive-post-date">01/21</span><a class="archive-post-title" href= "/blog/21/CS373-Week-1/" >CS373 Week 1</a>
        </li>
    
    </div>
  </div>
        <div class="sidebar-tags">
    <div class="sidebar-tags-name">
    
        <span class="sidebar-tag-name"><a href= "#">CS373</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human-in-the-Loop</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Reinforcement Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">IJCAI Survey 2019</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Human Feedback</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Deep Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Preferences Learning</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">COACH</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">Explainable RL</a></span>
    
        <span class="sidebar-tag-name"><a href= "#">XAI</a></span>
    
    </div>
    <div class="tag-load-fail" style="display: none; color: #ccc; font-size: 0.6rem;">
    缺失模块。<br/>
    1、请确保node版本大于6.2<br/>
    2、在博客根目录（注意不是archer根目录）执行以下命令：<br/>
    <span style="color: #f75357; font-size: 1rem; line-height: 2rem;">npm i hexo-generator-json-content --save</span><br/>
    3、在根目录_config.yml里添加配置：
    <pre style="color: #787878; font-size: 0.6rem;">
jsonContent:
  meta: false
  pages: false
  posts:
    title: true
    date: true
    path: true
    text: false
    raw: false
    content: false
    slug: false
    updated: false
    comments: false
    link: false
    permalink: false
    excerpt: false
    categories: false
    tags: true</pre>
    </div> 
    <div class="sidebar-tag-list"></div>
</div>
    </div>
</div> 
    <script>
    var jsInfo = {
        root: '/blog/'
    }
</script>
    <!-- 不蒜子  -->
    
    <!-- CNZZ统计  -->
    
    </div>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>


